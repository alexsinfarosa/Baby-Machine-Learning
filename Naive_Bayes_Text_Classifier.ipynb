{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes text classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will attempt to illustrate how to apply the \"Naive Bayes\" methodology to construct a simple text classifier.\n",
    "\n",
    "The data folder contains 71 files:\n",
    "\n",
    "- thirty files labelled p0.txt -- p29.txt, with text extracted from ten recent news articles about politics\n",
    "- thirty files labelled s0.txt -- s29.txt, with text extracted from ten recent news articles about sports\n",
    "- ten files labelled t0.txt -- t9.txt, which will be the \"test set\"\n",
    "\n",
    "The data were pulled down on 10 Feb 2016, so dominated by Iowa and New Hampshire primaries on the politics side, and by the SuperBowl on the sports side.\n",
    "\n",
    "We will train a binary classifier (politics/sports) based on the first sixty \"training\" files (p0.txt -- p29.txt and s0.txt -- s29.txt), and test it on the ten \"test\" files (t0.txt -- t9.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the texts into our notebook.\n",
    "Their contents can be imported into three python lists as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptexts=[open('data/p{}.txt'.format(i)).read() for i in range(30)]\n",
    "stexts=[open('data/s{}.txt'.format(i)).read() for i in range(30)]\n",
    "ttexts=[open('data/t{}.txt'.format(i)).read() for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, the file p0.txt is now available to us as ptexts[0], in fact we could try to view its content by typing:  `print(ptexts[0])`\n",
    "\n",
    "At this point we need to split the texts into something like words. For this purpose, characters are all made lowercase, then all strings of letters a-z plus apostrophes can be extracted as a list. For example, for the first political text ptexts[0], and using a regular expresion that finds all strings with a-z and ':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(\"[a-z']+\",ptexts[0].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.findall(\"[a-z']+\",txt)` just finds all contiguous strings made up of the characters a-z or ', and returns them as a list.\n",
    "\n",
    "This will return a list of all the 'words' in that document. For the time being, we only want to count the number of documents in which a word occurs (not the number of occurrences within the document), so we apply set() to the above list so that each word only appears once. Finally, it's convenient to use the Counter object (http://docs.python.org/2/library/collections.html), which is just a dictionary to accumulate counts for the words. The result is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(set(re.findall(\"[a-z']+\",ptexts[0].lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define two counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to accumulate the number counts (number of documents in which word occurs), we'll define two counters, n_p for the number counts of words in the politics training set, and n_s for the number counts in the sports training set, by iterating through the associated training sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_p = np.sum([Counter(set(re.findall(\"[a-z']+\",txt.lower()))) for txt in ptexts])\n",
    "n_s = np.sum([Counter(set(re.findall(\"[a-z']+\",txt.lower()))) for txt in stexts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"-1\">[Note: the above requires having numpy functions loaded in the primary namespace, e.g., using `%pylab inline` (executes `from numpy import *`, populating the namespace with numpy functions). We could also use instead, as shown above, `import numpy as np`, but then we need `np.sum()` in the above since the standard `sum()` does not support adding `Counter()` objects. Something to remember is that each `Counter()` is a dictionary whose keys are words, and associated values are their number counts, so the `sum()` has to be smart enough to know that adding `Counter()`s means taking the set union of their keys and adding the count values of any coincident ones. The generic python `sum()` does not do this (instead generates a \"TypeError: unsupported operand type(s) for +\"), but the numpy `sum()` does work properly for this.]</font>\n",
    "\n",
    "The Counter object has a convenient 'most_common' method, so we can look at the numbers for the most frequent words (where 30 means they occurred in all thirty training documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('his', 30),\n",
       "  ('of', 30),\n",
       "  ('be', 30),\n",
       "  ('for', 30),\n",
       "  ('the', 30),\n",
       "  ('and', 30),\n",
       "  ('that', 30),\n",
       "  ('from', 30),\n",
       "  ('but', 30),\n",
       "  ('on', 30)],\n",
       " [('be', 30),\n",
       "  ('the', 30),\n",
       "  ('all', 30),\n",
       "  ('was', 30),\n",
       "  ('at', 30),\n",
       "  ('that', 30),\n",
       "  ('but', 30),\n",
       "  ('on', 30),\n",
       "  ('a', 30),\n",
       "  ('and', 30)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_p.most_common(10),n_s.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the classifier \n",
    "\n",
    "The only thing left at this point is to write the classifier code to implement the Naive Bayes classifier. The formula is the following:\n",
    "\n",
    "$$\n",
    "p({\\rm politics}\\ |\\ words) = \\frac{p(words\\ |\\ {\\rm politics})\\,p({\\rm politics})}\n",
    "{p(words\\ |\\ {\\rm politics})\\,p({\\rm politics})+p(words\\ |\\ {\\rm sports})\\,p({\\rm sports})}\n",
    "\\approx\\frac{\\prod_i p(w_i\\ |\\ {\\rm politics})}{\\prod_i p(w_i\\ |\\ {\\rm politics})+\\prod_i p(w_i\\ |\\ {\\rm sports})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_train=30.\n",
    "s_train=30.\n",
    "def bayes_classifier(txt):\n",
    "    p=1.\n",
    "    s=1.\n",
    "    word_counts=Counter(re.findall(\"[a-z']+\",txt.lower()))\n",
    "    # We need to account for stopwords\n",
    "    keywords=[w for w,c in word_counts.most_common() if n_p[w]< 26 and n_s[w]<26][:30]\n",
    "    print('\\tbased on \"{} ...\":'.format(', '.join(keywords[:9])))\n",
    "    for word in keywords:\n",
    "        # handling words in the test documents that haven't been seen before (\".1 smoothing\")\n",
    "        if word not in n_p: n_p[word]=.1\n",
    "        if word not in n_s: n_s[word]=.1\n",
    "        p *= n_p[word]/p_train\n",
    "        s *= n_s[word]/s_train\n",
    "    prob=p/(p+s)\n",
    "    if prob >= .5:\n",
    "        return 'POLITICS',prob\n",
    "    else:\n",
    "        return 'SPORT',1-prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbased on \"rubio, him, he's, rubio's, there's, could, question, suggest, florida ...\":\n",
      "t0.txt: ('POLITICS', 0.9999999164340486)\n",
      "\tbased on \"warriors, i, ever, spurs, about, greatest, were, been, maybe ...\":\n",
      "t1.txt: ('SPORT', 1.0)\n",
      "\tbased on \"saban, alabama, football, coach, college, team, will, state, national ...\":\n",
      "t2.txt: ('SPORT', 1.0)\n",
      "\tbased on \"trump, will, iowa, cruz, gop, trump's, republican, rubio, there ...\":\n",
      "t3.txt: ('POLITICS', 1.0)\n",
      "\tbased on \"sanders, democratic, left, will, win, party, people, s, lose ...\":\n",
      "t4.txt: ('POLITICS', 1.0)\n",
      "\tbased on \"georgia, world, russia, rugby, cup, team, georgia's, war, lelos ...\":\n",
      "t5.txt: ('SPORT', 0.9999998466773602)\n",
      "\tbased on \"bush, campaign, hampshire, about, jeb, trump, he's, him, how ...\":\n",
      "t6.txt: ('POLITICS', 0.9999999999996046)\n",
      "\tbased on \"curry, play, season, after, been, warriors, five, better, team ...\":\n",
      "t7.txt: ('SPORT', 0.9999999999998045)\n",
      "\tbased on \"scholarships, athletes, college, players, scholarship, year, could, her, she ...\":\n",
      "t8.txt: ('SPORT', 0.9999999976566906)\n",
      "\tbased on \"golf, woods, tiger, no, best, championship, two, age, player ...\":\n",
      "t9.txt: ('SPORT', 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('t{}.txt:'.format(i),bayes_classifier(ttexts[i]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
